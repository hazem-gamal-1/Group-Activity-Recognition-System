{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9198495,"sourceType":"datasetVersion","datasetId":5561212}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Data_utils_file:","metadata":{}},{"cell_type":"code","source":"%%writefile data_utils.py\n\n\nimport cv2\nimport os\nimport pickle\nimport matplotlib.pyplot as plt\nfrom typing import List\nimport torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom PIL import Image\nimport random\nfrom torchvision.transforms import (Compose, Resize, CenterCrop, ToTensor, Normalize, \n                                    RandomAffine, ColorJitter, RandomHorizontalFlip, \n                                    RandomVerticalFlip, RandomRotation, RandomPerspective, \n                                    GaussianBlur, RandomGrayscale, RandomErasing)\nfrom torchvision import  transforms\n\nfrom torchvision.transforms import Compose, RandomAffine, ColorJitter, RandomRotation, RandomPerspective, GaussianBlur, RandomAdjustSharpness, RandomPosterize\n\n\n\nclass BoxInfo:\n    def __init__(self,line):\n        words=line.split()\n        self.category = words.pop()\n        words = [int(string) for string in words]\n        self.player_ID = words[0]\n        del words[0]\n        x1, y1, x2, y2, frame_ID, lost, grouping, generated = words\n        self.box = x1, y1, x2, y2\n        self.frame_ID = frame_ID\n        self.lost = lost\n        self.grouping = grouping\n        self.generated = generated\n\n\n\n\ndef load_tracking_annot(path):\n    with open(path, 'r') as file:\n        player_boxes = {idx:[] for idx in range(12)}\n        frame_boxes_dct = {}\n\n        for idx, line in enumerate(file):\n            box_info = BoxInfo(line)\n            if box_info.player_ID > 11:\n                continue\n            player_boxes[box_info.player_ID].append(box_info)\n\n        # let's create view from frame to boxes\n        for player_ID, boxes_info in player_boxes.items():\n            # let's keep the middle 9 frames only (enough for this task empirically)\n            boxes_info = boxes_info[5:]\n            boxes_info = boxes_info[:-6]\n            boxes_info = boxes_info[5:6]\n    \n\n            \n            for box_info in boxes_info:\n                if box_info.frame_ID not in frame_boxes_dct:\n                    frame_boxes_dct[box_info.frame_ID] = []\n                    \n                frame_boxes_dct[box_info.frame_ID].append(box_info)\n\n        return frame_boxes_dct\n\n\n\ndef vis_clip(annot_path, video_dir):\n    frame_boxes_dct = load_tracking_annot(annot_path)\n    font = cv2.FONT_HERSHEY_SIMPLEX\n\n    for frame_id, boxes_info in frame_boxes_dct.items():\n        img_path = os.path.join(video_dir, f'{frame_id}.jpg')\n        print(img_path)\n        image = cv2.imread(img_path)\n\n        if image is None:\n            print(f\"Image not found: {img_path}\")\n            continue\n\n        for box_info in boxes_info:\n            x1, y1, x2, y2 = box_info.box\n            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(image, box_info.category, (x1, y1 - 10), font, 0.5, (0, 255, 0), 2)\n\n        # Display the frame inline\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.axis('off')\n        plt.show()\n\n\ndef load_video_annot(video_annot):\n       with open(video_annot, 'r') as file:\n        clip_category_dct = {}\n        for line in file:\n            items = line.strip().split(' ')[:2]\n            clip_dir = items[0].replace('.jpg', '')\n            clip_category_dct[clip_dir] = items[1]\n\n        return clip_category_dct\n\n\ndef load_volleyball_dataset(videos_root, annot_root):\n    videos_dirs = os.listdir(videos_root)\n    videos_dirs.sort()\n    videos_annot = {}\n\n    for idx, video_dir in enumerate(videos_dirs):\n        video_dir_path = os.path.join(videos_root, video_dir)\n        if not os.path.isdir(video_dir_path):\n            continue\n\n        print(f'{idx+1}/{len(videos_dirs)} - Processing Dir {video_dir_path}')\n        video_annot = os.path.join(video_dir_path, 'annotations.txt')\n        clip_category_dct = load_video_annot(video_annot)\n\n        clips_dir = os.listdir(video_dir_path)\n        clips_dir.sort()\n        clip_annot = {}\n\n        for clip_dir in clips_dir:\n            clip_dir_path = os.path.join(video_dir_path, clip_dir)\n            if not os.path.isdir(clip_dir_path):\n                continue\n\n            \n            assert clip_dir in clip_category_dct\n            annot_file = os.path.join(annot_root, video_dir, clip_dir, f'{clip_dir}.txt')\n            frame_boxes_dct = load_tracking_annot(annot_file)\n\n            clip_annot[clip_dir] = {\n                'category': clip_category_dct[clip_dir],\n                'frame_boxes_dct': frame_boxes_dct\n            }\n\n\n        videos_annot[video_dir] = clip_annot\n\n    return videos_annot\n\n\n\n\n\ndef create_class_mapping(dataset):\n\n    # Collect all the unique classes\n    classes = set(clip['category'] for video_index in dataset.keys() \n                                  for clip_id, clip in dataset[video_index].items())\n    \n    # Sort classes to ensure deterministic ordering\n    classes = sorted(classes)\n    \n    # Create the class-to-index mapping\n    class_to_index = {clip_class: idx for idx, clip_class in enumerate(classes)}\n    \n    return class_to_index\n\n\n\nclass Volleyball_training_dataset_baseline_1(Dataset):\n    def __init__(self, dataset, videos_path, videos_indices):\n        \n        self.videos_path = videos_path\n        \n        # Filter the dataset to only include the specified video indices.\n        self.dataset = {key: value for key, value in dataset.items() if key in videos_indices}\n        \n        # Generate or use the provided class mapping.\n   \n        self.class_to_index = create_class_mapping(self.dataset)\n       \n       \n        \n   \n        self.pre_transform = Compose([\n            Resize((256, 256)),\n            CenterCrop((224, 224))\n        ])\n      \n\n        \n        self.post_transform = Compose([\n            ToTensor(),\n            Normalize(mean=[0.485, 0.456, 0.406],\n                      std=[0.229, 0.224, 0.225])\n        ])\n\n\n        \n     \n\n        self.augmentation_transforms = [\n            # Pipeline 1: Moderate affine transform with color jitter.\n            Compose([\n                RandomAffine(degrees=10, translate=(0.0, 0.1)),\n                ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05)\n            ]),\n            # Pipeline 2: Rotation with perspective distortion and a mild blur.\n            Compose([\n                RandomRotation(degrees=10),\n                RandomPerspective(distortion_scale=0.3, p=0.5),\n                GaussianBlur(kernel_size=3)\n            ]),\n            # Pipeline 3: Increased rotation angle with color jitter and a sharpness adjustment.\n            Compose([\n                RandomRotation(degrees=20),\n                ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),\n                RandomAdjustSharpness(sharpness_factor=2, p=0.5)\n            ]),\n            # Pipeline 4: Posterization with color jitter for a different style.\n            Compose([\n                RandomPosterize(bits=4),\n                ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05)\n            ])\n        \n             ,             # Pipeline 5: Grayscale conversion\n            Compose([\n                RandomGrayscale(p=0.2),\n            ])\n            ]\n\n\n      \n        self.samples = [] \n        for video_index, clips in self.dataset.items():\n            for clip_id, clip_data in clips.items():\n                # Get the label as an integer.\n                label = self.class_to_index[clip_data['category']]\n                # For every frame in the clip...\n                for frame_id in clip_data['frame_boxes_dct'].keys():\n                    frame_path = os.path.join(self.videos_path, video_index, clip_id, f\"{frame_id}.jpg\")\n                    try:\n                        image = Image.open(frame_path).convert(\"RGB\")\n                    except Exception as e:\n                        print(f\"Error loading image {frame_path}: {e}\")\n                        continue\n                    \n                    # First, apply the pre_transform (e.g., Resize, CenterCrop) to obtain a consistent PIL image.\n                    pre_image = self.pre_transform(image)\n                    # Compute the original sample.\n                    original_tensor = self.post_transform(pre_image)\n                    self.samples.append((original_tensor, label))\n\n                    \n                    \n                    for aug in self.augmentation_transforms:\n                        for _ in range(2):\n                            aug_image = aug(pre_image)          \n                            aug_tensor = self.post_transform(aug_image)  \n                            self.samples.append((aug_tensor, label))\n                        \n        print(f\"Training Dataset contains {len(self.samples)} samples.\")\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        return self.samples[idx]\n\n\n\n\n\ndef get_val_transform():\n    preprocess = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.CenterCrop((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n    return preprocess\n\n\nclass Volleyball_testing_dataset_baseline_1(Dataset):\n    def __init__(self, dataset, videos_path, videos_indices, transform=None, class_to_index=None):\n        self.dataset = dataset\n        self.videos_path = videos_path\n        self.videos_indices = videos_indices\n        self.transform = transform\n        \n        # Filter dataset to only include the specified video indices.\n        self.dataset = {key: value for key, value in self.dataset.items() if key in self.videos_indices}\n        \n        # Generate or use the provided class mapping.\n        if class_to_index is None:\n            self.class_to_index = create_class_mapping(self.dataset)\n        else:\n            self.class_to_index = class_to_index\n\n        self.indices = []\n        for video_index in self.dataset.keys():\n            for clip_id in self.dataset[video_index].keys():\n                frame_ids = list(self.dataset[video_index][clip_id]['frame_boxes_dct'].keys())\n                try:\n                    # Try converting frame_ids to integers for proper numeric sorting.\n                    frame_ids = sorted(frame_ids, key=lambda x: int(x))\n                except ValueError:\n                    # If conversion fails, sort as strings.\n                    frame_ids = sorted(frame_ids)\n                \n                if frame_ids:\n                    # Calculate the middle index. For an even number of frames, this chooses the lower middle.\n                    middle_index = len(frame_ids) // 2\n                    middle_frame = frame_ids[middle_index]\n                    # Append the tuple (video_index, clip_id, middle_frame)\n                    self.indices.append((video_index, clip_id, middle_frame))\n     \n        print(f\"Class mapping for testing: {self.class_to_index}\")\n            \n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        video_index, clip_id, frame_id = self.indices[idx]\n        \n        # Get the class of the clip (label) and map it to an integer index.\n        clip_class = self.dataset[video_index][clip_id]['category']\n        clip_class_idx = self.class_to_index[clip_class]\n        clip_class_idx = torch.tensor(clip_class_idx, dtype=torch.long)\n        \n        # Construct the path to the frame image.\n        frame_path = os.path.join(self.videos_path, video_index, clip_id, f\"{frame_id}.jpg\")\n        \n        # Load the frame image and convert it to RGB for consistency.\n        frame = Image.open(frame_path).convert(\"RGB\")\n        \n        if self.transform:\n            frame = self.transform(frame)\n            \n        return frame, clip_class_idx\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T02:24:37.773443Z","iopub.execute_input":"2025-02-24T02:24:37.774386Z","iopub.status.idle":"2025-02-24T02:24:37.798499Z","shell.execute_reply.started":"2025-02-24T02:24:37.774301Z","shell.execute_reply":"2025-02-24T02:24:37.797029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model_utils_file :","metadata":{}},{"cell_type":"code","source":"%%writefile modeling_utils.py\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nimport torch.optim as optim\nimport numpy\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\ndef Get_device():\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\ndef Get_model(num_classes, device):\n    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n\n    # Enable gradient updates for all layers\n    for param in model.parameters():\n        param.requires_grad = True\n\n    for param in model.conv1.parameters():\n        param.requires_grad = False\n    for param in model.bn1.parameters():\n        param.requires_grad = False\n\n    # Modify the last fully connected layer for num_classes\n    model.fc = nn.Sequential(\n    nn.Dropout(p=0.5),  # Dropout with 50% probability\n    nn.Linear(model.fc.in_features, num_classes)\n    )\n\n    # Move model to the specified device\n    model = model.to(device)\n    model.train()\n    return model\n\n\n\n\n\n\n\n\ndef train(model, dataloader,val_loader,optimizer, criterion, device,class_to_index_dict,epochs=50):\n    model.train()\n    train_losses = []\n    train_accuracies = []\n    val_losses=[]\n    val_accuracies=[]\n\n    # Track the best validation accuracy and best model weights\n    best_val_accuracy = 0.0\n    \n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n       \n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n                 \n            \n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n            # Calculate running loss\n            running_loss += loss.item()\n\n            # Calculate accuracy\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        # Record epoch statistics\n        epoch_loss = running_loss / len(dataloader)\n        epoch_accuracy = 100 * correct / total\n\n        \n        train_losses.append(epoch_loss)\n        train_accuracies.append(epoch_accuracy)\n\n        \n    \n        val_loss,val_accuracy=evaluate_model(model, val_loader,device,criterion,\"Validation\",list(class_to_index_dict.keys()))\n        val_losses.append(val_loss)\n        val_accuracies.append(val_accuracy)\n\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            torch.save(model.state_dict(), \"best_model.pt\")\n            print(f\"Best model updated at epoch {epoch+1} with validation accuracy: {best_val_accuracy:.2f}%\")\n        \n\n        \n        # Print statistics for the current epoch\n        print(f\"Epoch {epoch+1}/{epochs}, Training_Loss: {epoch_loss:.4f}, Training_Accuracy:{epoch_accuracy:.2f}%\")\n        print(f\"Epoch {epoch+1}/{epochs}, Validation_loss:{val_loss:.4f}, Validation_Accuracy:{val_accuracy:.2f}%\")\n    return  train_losses ,train_accuracies,val_losses,val_accuracies\n\n\n\n    \ndef plot_losses_and_accuracies(train_losses, val_losses, train_accuracies, val_accuracies):\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(12, 6))\n    \n   \n    train_color = 'red'\n    val_color = 'blue'\n    \n\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label='Training Loss', color=train_color, marker='o')\n    plt.plot(epochs, val_losses, label='Validation Loss', color=val_color, linestyle='dashed', marker='s')\n    plt.xticks(epochs)  \n    plt.title('Loss over Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, label='Training Accuracy', color=train_color, marker='o')\n    plt.plot(epochs, val_accuracies, label='Validation Accuracy', color=val_color, linestyle='dashed', marker='s')\n    plt.xticks(epochs)  \n    plt.title('Accuracy over Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\ndef evaluate_model(model, data_loader, device, criterion,mode=\"Validation\",class_names=None):\n    all_preds = []\n    all_labels = []\n    total_loss = 0.0\n    batch_count = 0\n    \n    model.eval()  # Set model to evaluation mode\n    with torch.no_grad():  # Disable gradient computation\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)  # Forward pass\n            \n            # Compute loss for the batch\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            batch_count += 1\n            \n            # For multi-class classification, get the class with the highest probability\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            \n            # Collect predictions and true labels\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate average loss over all batches\n    avg_loss = total_loss / batch_count if batch_count > 0 else 0.0\n\n    # Convert lists to numpy arrays for metric computations\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n    \n    # Print statistics\n    if mode==\"Testing\":\n         print(f\"Loss: {avg_loss:.4f}\")\n         print(f\"Accuracy: {accuracy * 100:.2f}%\")\n        \n         if class_names:\n            print(\"\\nClassification Report:\")\n            print(classification_report(all_labels, all_preds, target_names=class_names))\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n                        xticklabels=class_names if class_names else np.unique(all_labels), \n                        yticklabels=class_names if class_names else np.unique(all_labels))\n            plt.xlabel(\"Predicted Labels\")\n            plt.ylabel(\"True Labels\")\n            plt.title(\"Confusion Matrix\")\n            plt.show()\n\n    model.train()  # Set the model back to training mode before returning\n    return avg_loss, accuracy*100\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T02:24:37.811263Z","iopub.execute_input":"2025-02-24T02:24:37.812077Z","iopub.status.idle":"2025-02-24T02:24:37.825025Z","shell.execute_reply.started":"2025-02-24T02:24:37.812020Z","shell.execute_reply":"2025-02-24T02:24:37.823781Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Main File\n  - ### step 1: imort libraries\n  - ### step 2: Loading Data\n  - ### step 3: Training\n  - ### step 4: Testing","metadata":{}},{"cell_type":"markdown","source":"# step 1: imorting libraries","metadata":{}},{"cell_type":"code","source":"import importlib\nfrom data_utils import Volleyball_testing_dataset_baseline_1\nfrom torch.utils.data import Dataset,DataLoader\n\n%run data_utils.py\n%run modeling_utils.py\n\nimport data_utils\nimport modeling_utils\n\n\nimportlib.reload(data_utils)\nimportlib.reload(modeling_utils)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T02:24:37.827383Z","iopub.execute_input":"2025-02-24T02:24:37.827842Z","iopub.status.idle":"2025-02-24T02:24:45.185735Z","shell.execute_reply.started":"2025-02-24T02:24:37.827800Z","shell.execute_reply":"2025-02-24T02:24:45.184506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# step 2: Loading Data","metadata":{}},{"cell_type":"code","source":"videos_path=f'/kaggle/input/volleyball/volleyball_/videos'\n\nannots_path=f'/kaggle/input/volleyball/volleyball_tracking_annotation/volleyball_tracking_annotation'\n\ndataset=load_volleyball_dataset(videos_path,annots_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T02:24:45.187114Z","iopub.execute_input":"2025-02-24T02:24:45.187643Z","iopub.status.idle":"2025-02-24T02:25:31.375513Z","shell.execute_reply.started":"2025-02-24T02:24:45.187607Z","shell.execute_reply":"2025-02-24T02:25:31.374494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ntrain_ids= [\"1\", \"3\", \"6\", \"7\", \"10\", \"13\", \"15\", \"16\", \"18\", \"22\", \"23\", \"31\", \"32\", \"36\", \"38\", \"39\", \"40\", \"41\", \"42\", \"48\", \"50\", \"52\", \"53\", \"54\"]\n\nTraining_dataset=Volleyball_training_dataset_baseline_1(dataset,videos_path,train_ids)\n\ntrain_loader = DataLoader(Training_dataset, batch_size=64, shuffle=True)\n\n\n\n\n\n# Evaluate the model on validation_set\nval_transform=get_val_transform()\n\nval_ids = [\"0\", \"2\", \"8\", \"12\", \"17\", \"19\", \"24\", \"26\", \"27\", \"28\", \"30\", \"33\", \"46\", \"49\", \"51\"]\n\nval_dataset=Volleyball_testing_dataset_baseline_1(dataset,videos_path,val_ids,val_transform,Training_dataset.class_to_index)\n\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T02:25:31.376979Z","iopub.execute_input":"2025-02-24T02:25:31.377800Z","iopub.status.idle":"2025-02-24T02:28:35.548644Z","shell.execute_reply.started":"2025-02-24T02:25:31.377753Z","shell.execute_reply":"2025-02-24T02:28:35.547776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# step 3: Training\n","metadata":{}},{"cell_type":"code","source":"# Define the loss function (CrossEntropyLoss for classification)\ndevice=Get_device()\n\nmodel = Get_model(\n    num_classes=8,\n    device=device)\n\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n\n# Set up the AdamW optimizer\noptimizer = optim.AdamW([\n    {'params': model.layer1.parameters(), 'lr': 1e-4},\n    {'params': model.layer2.parameters(), 'lr': 1e-4},\n    {'params': model.layer3.parameters(), 'lr': 1e-4},\n    {'params': model.layer4.parameters(), 'lr': 1e-4},\n    {'params': model.fc.parameters(),    'lr': 1e-4}\n], \n  \n    weight_decay=1e-4  # Proper decoupled weight decay\n)\n\n\ntrain_loss,train_accuracy,val_loss,val_accuracy=train(model, train_loader, val_loader,optimizer, criterion, device,Training_dataset.class_to_index, epochs=35)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T02:28:35.550446Z","iopub.execute_input":"2025-02-24T02:28:35.550742Z","iopub.status.idle":"2025-02-24T04:51:37.587980Z","shell.execute_reply.started":"2025-02-24T02:28:35.550688Z","shell.execute_reply":"2025-02-24T04:51:37.587082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training vs Validation Curves ","metadata":{}},{"cell_type":"code","source":"plot_losses_and_accuracies(train_loss,val_loss,train_accuracy,val_accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T04:51:37.589658Z","iopub.execute_input":"2025-02-24T04:51:37.590134Z","iopub.status.idle":"2025-02-24T04:51:38.344538Z","shell.execute_reply.started":"2025-02-24T04:51:37.590091Z","shell.execute_reply":"2025-02-24T04:51:38.343723Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# step 4:Testing ","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"best_model.pt\"))\n\n# Move the model to the correct device (if not already done in Get_model)\nmodel.to(device)\n\n\ntest_ids = [\"4\",\"5\",\"9\",\"11\",\"14\",\"20\",\"21\", \"25\", \"29\", \"34\", \"35\", \"37\", \"43\", \"44\" ,\"45\" ,\"47\"]\n\ntest_dataset=Volleyball_testing_dataset_baseline_1(dataset,videos_path,test_ids,val_transform,Training_dataset.class_to_index)\n\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(\"Evaluating the model on the Testing set...\")\n\nevaluate_model(model, test_loader,device,criterion,\"Testing\",list(Training_dataset.class_to_index.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T04:51:38.345535Z","iopub.execute_input":"2025-02-24T04:51:38.345886Z","iopub.status.idle":"2025-02-24T04:52:16.748461Z","shell.execute_reply.started":"2025-02-24T04:51:38.345858Z","shell.execute_reply":"2025-02-24T04:52:16.747595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}